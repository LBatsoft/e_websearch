"""
é«˜çº§æŸ¥è¯¢åˆ†æå™¨ä½¿ç”¨ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ä¸åŒçš„æŸ¥è¯¢åˆ†ææ–¹æ³•ï¼š
1. åŸºäºBERTçš„è¯­ä¹‰åˆ†æ
2. åŸºäºæœºå™¨å­¦ä¹ çš„åˆ†ç±»
3. åŸºäºLLMçš„æ·±åº¦åˆ†æ
4. æ··åˆå¼å¤šæ¨¡å‹é›†æˆ
"""

import asyncio
import json
import sys
import os
from pathlib import Path
from typing import List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from core.agent.advanced_query_analyzer import (
        BERTQueryAnalyzer,
        MLQueryClassifier,
        LLMQueryAnalyzer,
        HybridQueryAnalyzer,
        QueryAnalysisResult
    )
    from core.llm_enhancer import LLMEnhancer
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬ï¼Œæˆ–è€…å®‰è£…é¡¹ç›®ä¾èµ–")
    print("è¿è¡Œå‘½ä»¤: cd /Users/morein/work/python/e_websearch && python examples/advanced_query_analysis_example.py")
    sys.exit(1)


async def demo_bert_analyzer():
    """æ¼”ç¤ºBERTæŸ¥è¯¢åˆ†æå™¨"""
    print("=" * 60)
    print("ğŸ¤– BERTæŸ¥è¯¢åˆ†æå™¨æ¼”ç¤º")
    print("=" * 60)
    
    analyzer = BERTQueryAnalyzer()
    
    if not analyzer.is_available():
        print("âŒ BERTåˆ†æå™¨ä¸å¯ç”¨ï¼ˆéœ€è¦å®‰è£…transformerså’Œtorchï¼‰")
        return
    
    test_queries = [
        "ChatGPT vs Claude å¯¹æ¯”åˆ†æ",
        "Pythonæœºå™¨å­¦ä¹ å…¥é—¨æ•™ç¨‹",
        "ä»€ä¹ˆæ˜¯åŒºå—é“¾æŠ€æœ¯",
        "æœ€æ–°äººå·¥æ™ºèƒ½å‘å±•è¶‹åŠ¿"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” åˆ†ææŸ¥è¯¢: {query}")
        
        # æŸ¥è¯¢ç¼–ç 
        vector = await analyzer.encode_query(query)
        if vector is not None:
            print(f"   ğŸ“Š è¯­ä¹‰å‘é‡ç»´åº¦: {vector.shape}")
            print(f"   ğŸ“Š å‘é‡å‰5ç»´: {vector[:5]}")
        
        # æ„å›¾åˆ†ç±»
        intent_scores = await analyzer.classify_intent(query)
        if intent_scores:
            print(f"   ğŸ¯ æ„å›¾åˆ†ç±»: {intent_scores}")


def demo_ml_classifier():
    """æ¼”ç¤ºæœºå™¨å­¦ä¹ åˆ†ç±»å™¨"""
    print("\n" + "=" * 60)
    print("ğŸ“Š æœºå™¨å­¦ä¹ åˆ†ç±»å™¨æ¼”ç¤º")
    print("=" * 60)
    
    classifier = MLQueryClassifier()
    
    if not classifier.available:
        print("âŒ MLåˆ†ç±»å™¨ä¸å¯ç”¨ï¼ˆéœ€è¦å®‰è£…scikit-learnï¼‰")
        return
    
    test_queries = [
        "ChatGPT vs Claude å¯¹æ¯”åˆ†æ",
        "Pythonæœºå™¨å­¦ä¹ å…¥é—¨æ•™ç¨‹", 
        "ä»€ä¹ˆæ˜¯åŒºå—é“¾æŠ€æœ¯",
        "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„å…·ä½“åº”ç”¨æ¡ˆä¾‹å’Œæœªæ¥å‘å±•å‰æ™¯åˆ†æ"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” åˆ†ææŸ¥è¯¢: {query}")
        
        # æŸ¥è¯¢ç±»å‹é¢„æµ‹
        query_type, type_conf = classifier.predict_query_type(query)
        print(f"   ğŸ“ æŸ¥è¯¢ç±»å‹: {query_type} (ç½®ä¿¡åº¦: {type_conf:.3f})")
        
        # å¤æ‚åº¦é¢„æµ‹
        complexity, comp_conf = classifier.predict_complexity(query)
        print(f"   ğŸ“ å¤æ‚åº¦: {complexity} (ç½®ä¿¡åº¦: {comp_conf:.3f})")
        
        # æ„å›¾é¢„æµ‹
        intent, intent_conf = classifier.predict_intent(query)
        print(f"   ğŸ¯ æ„å›¾: {intent} (ç½®ä¿¡åº¦: {intent_conf:.3f})")


async def demo_llm_analyzer():
    """æ¼”ç¤ºLLMæŸ¥è¯¢åˆ†æå™¨"""
    print("\n" + "=" * 60)
    print("ğŸ§  LLMæŸ¥è¯¢åˆ†æå™¨æ¼”ç¤º")
    print("=" * 60)
    
    # åˆå§‹åŒ–LLMå¢å¼ºå™¨
    llm_enhancer = LLMEnhancer()
    analyzer = LLMQueryAnalyzer(llm_enhancer)
    
    if not analyzer.is_available():
        print("âŒ LLMåˆ†æå™¨ä¸å¯ç”¨ï¼ˆéœ€è¦é…ç½®LLM APIï¼‰")
        return
    
    test_queries = [
        "ChatGPT vs Claude å¯¹æ¯”åˆ†æ",
        "å¦‚ä½•ç³»ç»Ÿå­¦ä¹ æ·±åº¦å­¦ä¹ å¹¶åº”ç”¨åˆ°å®é™…é¡¹ç›®ä¸­"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” åˆ†ææŸ¥è¯¢: {query}")
        
        # Chain-of-Thoughtåˆ†æ
        print("   ğŸ”— Chain-of-Thoughtåˆ†æ:")
        cot_result = await analyzer.analyze_with_cot(query)
        if cot_result:
            print(f"      {json.dumps(cot_result, indent=6, ensure_ascii=False)}")
        
        # Few-shotåˆ†æ
        print("   ğŸ¯ Few-shotåˆ†æ:")
        few_shot_result = await analyzer.analyze_with_few_shot(query)
        if few_shot_result:
            print(f"      {json.dumps(few_shot_result, indent=6, ensure_ascii=False)}")


async def demo_hybrid_analyzer():
    """æ¼”ç¤ºæ··åˆå¼æŸ¥è¯¢åˆ†æå™¨"""
    print("\n" + "=" * 60)
    print("ğŸ”„ æ··åˆå¼æŸ¥è¯¢åˆ†æå™¨æ¼”ç¤º")
    print("=" * 60)
    
    # åˆå§‹åŒ–æ··åˆåˆ†æå™¨
    llm_enhancer = LLMEnhancer()
    analyzer = HybridQueryAnalyzer(llm_enhancer)
    
    test_queries = [
        "ChatGPT vs Claude å¯¹æ¯”åˆ†æ",
        "Pythonæœºå™¨å­¦ä¹ å…¥é—¨æ•™ç¨‹",
        "ä»€ä¹ˆæ˜¯åŒºå—é“¾æŠ€æœ¯",
        "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„å…·ä½“åº”ç”¨æ¡ˆä¾‹å’Œæœªæ¥å‘å±•å‰æ™¯åˆ†æ",
        "å¦‚ä½•ç³»ç»Ÿå­¦ä¹ æ·±åº¦å­¦ä¹ å¹¶åº”ç”¨åˆ°å®é™…é¡¹ç›®ä¸­"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” åˆ†ææŸ¥è¯¢: {query}")
        
        # æ‰§è¡Œæ··åˆåˆ†æ
        result = await analyzer.analyze_query(query)
        
        print(f"   ğŸ“ æŸ¥è¯¢ç±»å‹: {result.query_type}")
        print(f"   ğŸ“ å¤æ‚åº¦: {result.complexity}")
        print(f"   ğŸ¯ æ„å›¾: {result.intent}")
        print(f"   ğŸ·ï¸  å®ä½“: {result.entities}")
        print(f"   ğŸ”‘ å…³é”®è¯: {result.keywords}")
        print(f"   ğŸ”„ éœ€è¦å¤šæ­¥: {result.requires_multiple_searches}")
        print(f"   ğŸ“Š ç½®ä¿¡åº¦åˆ†æ•°: {result.confidence_scores}")
        
        if result.suggested_refinements:
            print(f"   ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for i, refinement in enumerate(result.suggested_refinements, 1):
                print(f"      {i}. {refinement}")
        
        if result.semantic_features is not None:
            print(f"   ğŸ§  è¯­ä¹‰ç‰¹å¾ç»´åº¦: {result.semantic_features.shape}")
        
        if result.llm_analysis:
            print(f"   ğŸ¤– LLMåˆ†æå¯ç”¨: âœ…")


def compare_analysis_methods():
    """æ¯”è¾ƒä¸åŒåˆ†ææ–¹æ³•çš„ç‰¹ç‚¹"""
    print("\n" + "=" * 60)
    print("ğŸ“Š åˆ†ææ–¹æ³•å¯¹æ¯”")
    print("=" * 60)
    
    comparison_data = [
        {
            "æ–¹æ³•": "åŸºäºè§„åˆ™",
            "å‡†ç¡®æ€§": "ä¸­ç­‰",
            "é€Ÿåº¦": "å¾ˆå¿«",
            "èµ„æºæ¶ˆè€—": "å¾ˆä½",
            "æ‰©å±•æ€§": "æœ‰é™",
            "é€‚ç”¨åœºæ™¯": "ç®€å•æŸ¥è¯¢ã€å¿«é€Ÿå“åº”"
        },
        {
            "æ–¹æ³•": "æœºå™¨å­¦ä¹ ",
            "å‡†ç¡®æ€§": "è¾ƒé«˜",
            "é€Ÿåº¦": "å¿«",
            "èµ„æºæ¶ˆè€—": "ä¸­ç­‰",
            "æ‰©å±•æ€§": "è‰¯å¥½",
            "é€‚ç”¨åœºæ™¯": "ä¸­ç­‰å¤æ‚åº¦æŸ¥è¯¢ã€æ‰¹é‡å¤„ç†"
        },
        {
            "æ–¹æ³•": "BERTæ¨¡å‹",
            "å‡†ç¡®æ€§": "é«˜",
            "é€Ÿåº¦": "ä¸­ç­‰",
            "èµ„æºæ¶ˆè€—": "è¾ƒé«˜",
            "æ‰©å±•æ€§": "å¾ˆå¥½",
            "é€‚ç”¨åœºæ™¯": "è¯­ä¹‰ç†è§£ã€ç›¸ä¼¼åº¦è®¡ç®—"
        },
        {
            "æ–¹æ³•": "LLMåˆ†æ",
            "å‡†ç¡®æ€§": "å¾ˆé«˜",
            "é€Ÿåº¦": "è¾ƒæ…¢",
            "èµ„æºæ¶ˆè€—": "é«˜",
            "æ‰©å±•æ€§": "æå¥½",
            "é€‚ç”¨åœºæ™¯": "å¤æ‚æŸ¥è¯¢ã€æ·±åº¦ç†è§£"
        },
        {
            "æ–¹æ³•": "æ··åˆæ–¹å¼",
            "å‡†ç¡®æ€§": "å¾ˆé«˜",
            "é€Ÿåº¦": "ä¸­ç­‰",
            "èµ„æºæ¶ˆè€—": "ä¸­é«˜",
            "æ‰©å±•æ€§": "æå¥½",
            "é€‚ç”¨åœºæ™¯": "ç”Ÿäº§ç¯å¢ƒã€å…¨é¢åˆ†æ"
        }
    ]
    
    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    headers = list(comparison_data[0].keys())
    
    # è®¡ç®—åˆ—å®½
    col_widths = {}
    for header in headers:
        col_widths[header] = max(
            len(header),
            max(len(str(row[header])) for row in comparison_data)
        ) + 2
    
    # æ‰“å°è¡¨å¤´
    header_row = "|".join(header.center(col_widths[header]) for header in headers)
    print(header_row)
    print("-" * len(header_row))
    
    # æ‰“å°æ•°æ®è¡Œ
    for row in comparison_data:
        data_row = "|".join(str(row[header]).center(col_widths[header]) for header in headers)
        print(data_row)


def usage_recommendations():
    """ä½¿ç”¨å»ºè®®"""
    print("\n" + "=" * 60)
    print("ğŸ’¡ ä½¿ç”¨å»ºè®®")
    print("=" * 60)
    
    recommendations = [
        {
            "åœºæ™¯": "å¿«é€ŸåŸå‹å¼€å‘",
            "æ¨èæ–¹æ¡ˆ": "åŸºäºè§„åˆ™çš„åˆ†æå™¨",
            "ç†ç”±": "å®ç°ç®€å•ï¼Œå“åº”å¿«é€Ÿï¼Œæ— éœ€é¢å¤–ä¾èµ–"
        },
        {
            "åœºæ™¯": "ä¸­å°å‹åº”ç”¨",
            "æ¨èæ–¹æ¡ˆ": "æœºå™¨å­¦ä¹ åˆ†ç±»å™¨",
            "ç†ç”±": "å‡†ç¡®æ€§è¾ƒé«˜ï¼Œèµ„æºæ¶ˆè€—é€‚ä¸­ï¼Œæ˜“äºéƒ¨ç½²"
        },
        {
            "åœºæ™¯": "è¯­ä¹‰æœç´¢åº”ç”¨",
            "æ¨èæ–¹æ¡ˆ": "BERTæŸ¥è¯¢åˆ†æå™¨",
            "ç†ç”±": "è¯­ä¹‰ç†è§£èƒ½åŠ›å¼ºï¼Œæ”¯æŒå‘é‡åŒ–è¡¨ç¤º"
        },
        {
            "åœºæ™¯": "é«˜ç²¾åº¦è¦æ±‚",
            "æ¨èæ–¹æ¡ˆ": "LLMæŸ¥è¯¢åˆ†æå™¨",
            "ç†ç”±": "ç†è§£èƒ½åŠ›æœ€å¼ºï¼Œå¯å¤„ç†å¤æ‚æŸ¥è¯¢"
        },
        {
            "åœºæ™¯": "ç”Ÿäº§ç¯å¢ƒ",
            "æ¨èæ–¹æ¡ˆ": "æ··åˆå¼åˆ†æå™¨",
            "ç†ç”±": "ç»¼åˆå¤šç§æ–¹æ³•ä¼˜åŠ¿ï¼Œå‡†ç¡®æ€§å’Œé²æ£’æ€§æœ€ä½³"
        }
    ]
    
    for i, rec in enumerate(recommendations, 1):
        print(f"\n{i}. åœºæ™¯: {rec['åœºæ™¯']}")
        print(f"   æ¨èæ–¹æ¡ˆ: {rec['æ¨èæ–¹æ¡ˆ']}")
        print(f"   ç†ç”±: {rec['ç†ç”±']}")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ é«˜çº§æŸ¥è¯¢åˆ†æå™¨æ¼”ç¤º")
    print("=" * 60)
    
    try:
        # æ¼”ç¤ºå„ç§åˆ†æå™¨
        await demo_bert_analyzer()
        demo_ml_classifier()
        await demo_llm_analyzer()
        await demo_hybrid_analyzer()
        
        # æ–¹æ³•å¯¹æ¯”å’Œå»ºè®®
        compare_analysis_methods()
        usage_recommendations()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
